# External imports
import argparse
import numpy as np
from tensorflow.keras.callbacks import ModelCheckpoint
from music21 import converter, note, chord, instrument, stream

# Internal imports
import data
import models

# Constants
MODEL_LIST = ['LSTM']
DATA_MODES = ['Numeric']

if __name__ == "__main__":
    # Initialize argparse
    parser = argparse.ArgumentParser()

    # General arguments
    parser.add_argument('-m', '--model_type', help=f'Model Type', choices=MODEL_LIST, required=True)
    parser.add_argument('-n', '--name', help='Experiment name', required=True)
    parser.add_argument('-l', '--loss_function', help='Loss function to use', default='sparse_categorical_crossentropy')
    parser.add_argument('-o', '--optimizer', help='Optimizer to use', default='rmsprop')
    parser.add_argument('-b', '--batch_size', help='Batch size', type=int, default=64)
    parser.add_argument('-e', '--epochs', help='Number of epochs', type=int, default=200)
    parser.add_argument('-d', '--data_mode', help=f'How to encode the MIDI data', choices=DATA_MODES, default='Numeric')
    parser.add_argument('-s', '--seq_len', help='Length of input sequence to model', type=int, default=100)
    parser.add_argument('-p', '--data_path', help='Path to training data', default='./data/val.pkl')
    parser.add_argument('-w', '--weights_path', help='Path to directory to store weights', default='./weights/')

    # LSTM-Specific Arguments
    parser.add_argument('--lstm_size', help='Size of LSTM layers', type=int, default=512)
    parser.add_argument('--lstm_num_layers', help='Number of LSTM layers', type=int, default=3)
    parser.add_argument('--lstm_dropout_prob', help='LSTM dropout probability', type=float)
    parser.add_argument('--lstm_num_hidden_dense', help='Number of hidden dense layers', type=int, default=1)
    parser.add_argument('--lstm_hidden_dense_size', help='Size of hidden denses layers', type=int, default=256)
    parser.add_argument('--lstm_hidden_dense_activation', help='Activation for hidden dense layers', default='relu')

    # Parse arguments
    args = parser.parse_args()


    # Initialize dataset
    if args.data_mode == 'Numeric':
        dataset = data.MIDINumericDataset(path=args.data_path, sequence_len=args.seq_len)
        out_shape = 1


    # Preprocess data
    network_input, network_output = dataset.get_data()

    # Create model
    if args.model_type == 'LSTM':
        model = models.create_lstm(network_input[0].shape,
                                   out_shape,
                                   lstm_size = args.lstm_size,
                                   num_lstm_layers = args.lstm_num_layers,
                                   dropout_prob = args.lstm_dropout_prob,
                                   num_hidden_dense = args.lstm_num_hidden_dense,
                                   hidden_dense_size = args.lstm_hidden_dense_size,
                                   hidden_dense_activation = args.lstm_hidden_dense_activation,
                                   loss_function = args.loss_function,
                                   optimizer = args.optimizer)


    # Load weights
    filepath = './weights/lstm_initial-01-4.6047.hdf5'
    model.load_weights(filepath)

    # Choose a starting point
    start = np.random.randint(0, len(network_input)-1)

    int_to_note = dict((number, note) for number, note in enumerate(dataset.pitchnames))
    pattern = network_input[start]

    prediction_output = []

    nNotes = 500 # about 2 min
    # generate notes
    for note_index in range(nNotes):
        # Prepare prediction inputs
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(dataset.n_vocab)

        # Get prediction
        prediction = model.predict(prediction_input, verbose=0)

        # Randomly sample from prediction vector
        p = prediction.squeeze()
        print(f'Prediction: {prediction.shape}')
        print(f'p: {p.shape}')
        index = int(np.random.choice(dataset.n_vocab, 1, p=p/p.sum()))

        # Convert prediction to note
        result = int_to_note[index]
        prediction_output.append(result)
        pattern = np.append(pattern, index)
        pattern = pattern[1:len(pattern)]


    # turn predictions into notes

    offset = 0
    output_notes = []
    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)
        # increase offset each iteration so that notes do not stack
        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='test_output.mid')
